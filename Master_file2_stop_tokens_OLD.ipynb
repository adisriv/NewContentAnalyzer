{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import math\n",
    "import copy\n",
    "from wordcloud import WordCloud\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read articles from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = None\n",
    "filename = '52_articles(working).txt'\n",
    "folder_name = 'Local_dataset'\n",
    "with open(folder_name + '/' + filename) as json_file:  \n",
    "    articles = json.load(json_file)\n",
    "df = pd.DataFrame(articles)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "pattern = re.compile('^[$€]?[0-9]+(,[0-9]+)*.?[0-9]*[,.]?$')\n",
    "\n",
    "def matchesNum(term):\n",
    "    if re.match(pattern, term):\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def removePunctuation(term):\n",
    "    return term.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def updateNumbers(text):\n",
    "    split_text = text.split(' ')\n",
    "    updated_split_text=[]\n",
    "    term_num=len(split_text)\n",
    "    i = 0\n",
    "    while i < term_num:\n",
    "        if matchesNum(split_text[i]):\n",
    "        \n",
    "            if (i+1 < term_num) and (removePunctuation(split_text[i+1])=='million' or removePunctuation(split_text[i+1])=='billion'):\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))+removePunctuation(split_text[i+1])\n",
    "                i+=1\n",
    "            else:\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))\n",
    "            new_word = new_word.rstrip('.')\n",
    "            new_word = new_word.replace('.','dot')\n",
    "            new_word = new_word.replace('$','dol')\n",
    "            new_word = new_word.replace('€','eur')\n",
    "            updated_split_text.append(new_word)\n",
    "        else:\n",
    "            updated_split_text.append(split_text[i])\n",
    "        i+=1\n",
    "    return ' '.join(updated_split_text)\n",
    "\n",
    "def process_text(text):\n",
    "    updated_text = updateNumbers(text)\n",
    "    words = nltk.word_tokenize(updated_text)\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w.translate(str.maketrans('', '', string.punctuation)) for w in words]\n",
    "    words = [w for w in words if w not in nltk.corpus.stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if w.isalnum()]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Process \"body\" of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_processed'] = [process_text(b) for b in df['body']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Split articles (i.e. old articles vs new article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new =df.iloc[:1,:]\n",
    "# df_old = df.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] =pd.to_datetime(df['date'])\n",
    "df.index = df['date']\n",
    "df = df.sort_index(ascending=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCA:\n",
    "    \n",
    "    def __init__(self,depth=1,init_topics=-1):\n",
    "        self.depth = depth\n",
    "        self.init_topics = init_topics\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self,df):\n",
    "        include_stop_tokens = False\n",
    "        current_topics = self.init_topics\n",
    "        if current_topics == -1:\n",
    "            current_topics = min(20,min(math.ceil(df.shape[0]/3) + 1, df.shape[0]))\n",
    "            \n",
    "        for i in range(self.depth):\n",
    "            \n",
    "            #perfom fitting and transforming\n",
    "            X_train = None\n",
    "            vect = None\n",
    "            if not include_stop_tokens:\n",
    "                X_train,vect = self.remove_stop_tokens(current_topics,df)\n",
    "                include_stop_tokens = True\n",
    "            else:\n",
    "                vect = TfidfVectorizer(sublinear_tf=True,ngram_range=(1,3),max_df=0.7).fit(df['body_processed'])\n",
    "                X_train = vect.transform(df['body_processed'])\n",
    "                \n",
    "            lda = LatentDirichletAllocation(n_components=current_topics,random_state=0,max_iter=400,learning_method='batch')\n",
    "            lda.fit(X_train)\n",
    "            transformed = lda.transform(X_train)\n",
    "            \n",
    "            #save model\n",
    "            model = {}\n",
    "            model['lda'] = lda\n",
    "            model['transformed'] = transformed\n",
    "            model['topics'] = current_topics\n",
    "            model['vect'] = vect\n",
    "            model['X_train'] = X_train\n",
    "            model['df'] = df\n",
    "            self.models.append(model)\n",
    "            \n",
    "            topic_result = {}\n",
    "            if i != self.depth-1:\n",
    "                for i in range(len(transformed)):\n",
    "                    max_num = -99\n",
    "                    max_index = -99\n",
    "                    for j in range(len(transformed[i])):\n",
    "                        if transformed[i][j] >= max_num:\n",
    "                            max_num = transformed[i][j]\n",
    "                            max_index = j\n",
    "                    if max_index not in topic_result:\n",
    "                        topic_result[max_index] = []\n",
    "                    topic_result[max_index].append(i)\n",
    "                \n",
    "                index = df.shape[0]-1\n",
    "                subtopic = None\n",
    "                for t in topic_result:\n",
    "                    for a in topic_result[t]:\n",
    "                        if a == index:\n",
    "                            subtopic=t\n",
    "                            break\n",
    "\n",
    "                df = df.iloc[topic_result[subtopic]]\n",
    "                current_topics = min(20,min(math.ceil(df.shape[0]/3) + 1, df.shape[0]))\n",
    "                \n",
    "    def remove_stop_tokens(self,current_topics,df):\n",
    "        vect = TfidfVectorizer(sublinear_tf=True,ngram_range=(1,3),max_df=0.7).fit(df['body_processed'])\n",
    "        X_train = vect.transform(df['body_processed'])\n",
    "        lda = LatentDirichletAllocation(n_components=current_topics,random_state=0,max_iter=400,learning_method='batch')\n",
    "        lda.fit(X_train)\n",
    "        transformed = lda.transform(X_train)\n",
    "\n",
    "        topic_val = [0] * current_topics\n",
    "        for t in transformed:\n",
    "            for i in range(len(t)):\n",
    "                topic_val[i] += t[i]\n",
    "                \n",
    "        outlier_indexes = self.outlier_detection(topic_val)\n",
    "        \n",
    "        df_vec = pd.DataFrame(X_train.toarray())\n",
    "        df_vec.columns = vect.get_feature_names()\n",
    "        for topic_index in outlier_indexes:\n",
    "            top_words = self.get_top_words(lda,vect.get_feature_names(),topic_index)\n",
    "            df_vec.drop(top_words, axis=1, inplace=True)\n",
    "            \n",
    "        X_train_processed = scipy.sparse.csr_matrix(df_vec.values)\n",
    "        return (X_train_processed,vect)\n",
    "            \n",
    "    def get_top_words(self,lda_model, feature_names, topic,n_top_words=20):\n",
    "        word_distrib = lda_model.components_[topic]\n",
    "        sorted_index = word_distrib.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = []\n",
    "        for i in sorted_index:\n",
    "            top_words.append(feature_names[i])\n",
    "    \n",
    "        return top_words\n",
    "                \n",
    "    def outlier_detection(self,arr):\n",
    "        q1 = np.percentile(arr, 25)\n",
    "        q3 = np.percentile(arr, 75)\n",
    "        iqr = q3 - q1\n",
    "        floor = q1 - 1.5*iqr\n",
    "        ceiling = q3 + 1.5*iqr\n",
    "        outlier_indexes = []\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] < floor or arr[i] > ceiling:\n",
    "                outlier_indexes.append(i)\n",
    "        return outlier_indexes\n",
    "            \n",
    "    def stacked_barplot(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        \n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]\n",
    "            transformed = current_model['transformed']\n",
    "            transformed_copy = copy.deepcopy(transformed)\n",
    "            topics = current_model['topics']\n",
    "            \n",
    "            plt.figure()\n",
    "            x=range(0, topics)\n",
    "            prev = None\n",
    "            first = True\n",
    "            width = 0.5\n",
    "            bars = []\n",
    "            for i in range(len(transformed_copy)): \n",
    "                p = plt.bar(x, transformed_copy[i], width, bottom=prev)\n",
    "                if first:\n",
    "                    prev = transformed_copy[i]\n",
    "                    first = False\n",
    "                else:\n",
    "                    prev += transformed_copy[i]\n",
    "                bars.append(p)\n",
    "            if len(transformed_copy) <= 15:\n",
    "                articles_num = range(1,len(transformed_copy)+1)\n",
    "                plt.legend([p[0] for p in bars],[a for a in articles_num])\n",
    "                \n",
    "    def pyldavis(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "            \n",
    "        pyLDAvis.enable_notebook()\n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]\n",
    "            panel = pyLDAvis.sklearn.prepare(current_model['lda'], current_model['X_train'], current_model['vect'], mds='tsne')\n",
    "            display(panel)\n",
    "            \n",
    "    def topic_distribution(self,depth=-1):\n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        print(\"************************************************************\")\n",
    "        for z in range(depth):\n",
    "            current_model = self.models[z]\n",
    "            transformed = current_model['transformed']            \n",
    "            for i in range(len(transformed)):\n",
    "                max_num = -99\n",
    "                max_index = -99\n",
    "                for j in range(len(transformed[i])):\n",
    "                    if transformed[i][j] >= max_num:\n",
    "                        max_num = transformed[i][j]\n",
    "                        max_index = j\n",
    "                print('max topic of doc ' + str(i) + ' is: ' + str(max_index))\n",
    "                \n",
    "            print(\"*********************************************************\")\n",
    "            \n",
    "    def print_LDA_results(self,lda_model, feature_names, n_top_words=50):\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            message = \"Topic %d: \" % topic_idx\n",
    "            message += \" \".join([(\"*\" + feature_names[i] + \"*\")\n",
    "                                 for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "            print(message)\n",
    "            print()\n",
    "            \n",
    "    def print_topics(self,depth=-1):\n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        print(\"************************************************************\")\n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]\n",
    "            lda = current_model['lda']\n",
    "            vect = current_model['vect']\n",
    "            self.print_LDA_results(lda,vect.get_feature_names())\n",
    "            print(\"************************************************************\")\n",
    "            \n",
    "    def word_cloud(self,depth=-1):\n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        \n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]\n",
    "            df = current_model['df']\n",
    "            \n",
    "            df_old = df.iloc[:df.shape[0]-1]\n",
    "            words = \"\"\n",
    "            for text in df_old['body_processed']:\n",
    "                words+= \" \" + text\n",
    "            wordcloud = WordCloud().generate(words)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            \n",
    "            words = df.iloc[df.shape[0]-1]['body_processed']\n",
    "            wordcloud = WordCloud().generate(words)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.figure() \n",
    "            print(\"************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca = NCA(depth=2)\n",
    "nca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.stacked_barplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.topic_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.word_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
