{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import math\n",
    "import copy\n",
    "from wordcloud import WordCloud\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read articles from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = None\n",
    "filename = 'brexit_new.txt'\n",
    "folder_name = 'Master_dataset'\n",
    "\n",
    "with open(folder_name + '/' + filename) as json_file:  \n",
    "    articles = json.load(json_file)\n",
    "df = pd.DataFrame(articles)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pattern = re.compile('^[$€]?[0-9]+(,[0-9]+)*.?[0-9]*[,.]?$')\n",
    "\n",
    "# tests if a given term matches the reg ex above, which represents a number\n",
    "def matchesNum(term):\n",
    "    if re.match(pattern, term):\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "# removes punctuation from a term\n",
    "def removePunctuation(term):\n",
    "    return term.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# updates all numbers in a text file\n",
    "def updateNumbers(text):\n",
    "    split_text = text.split(' ')\n",
    "    updated_split_text=[]\n",
    "    term_num=len(split_text)\n",
    "    i = 0\n",
    "    \n",
    "    # tests if term matches a number, updating numerical term to be properly interpreted by the later text processing steps\n",
    "    # for example \"The company made $5.1 billion this year\" gets translated to \"The company made dol5dot1billion this year\"\n",
    "    while i < term_num:\n",
    "        if matchesNum(split_text[i]):\n",
    "        \n",
    "            if (i+1 < term_num) and (removePunctuation(split_text[i+1])=='million' or removePunctuation(split_text[i+1])=='billion'):\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))+removePunctuation(split_text[i+1])\n",
    "                i+=1\n",
    "            else:\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))\n",
    "            new_word = new_word.rstrip('.')\n",
    "            new_word = new_word.replace('.','dot')\n",
    "            new_word = new_word.replace('$','dol')\n",
    "            new_word = new_word.replace('€','eur')\n",
    "            updated_split_text.append(new_word)\n",
    "        else:\n",
    "            updated_split_text.append(split_text[i])\n",
    "        i+=1\n",
    "    return ' '.join(updated_split_text)\n",
    "\n",
    "# process text using lemmatization\n",
    "def process_text(text):\n",
    "    updated_text = updateNumbers(text)\n",
    "    words = nltk.word_tokenize(updated_text)\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w.translate(str.maketrans('', '', string.punctuation)) for w in words]\n",
    "    words = [w for w in words if w not in nltk.corpus.stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if w.isalnum()]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Process \"body\" of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_processed'] = [process_text(b) for b in df['body']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sort articles in ascending order (oldest to newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.index = df['date']\n",
    "df = df.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. New Content Analyzer (NCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCA:\n",
    "    \n",
    "    def __init__(self,depth=1,init_topics=-1):\n",
    "        self.depth = depth #how many times to perform lda\n",
    "        self.init_topics = init_topics #n of topics for the first lda. Set this if you have a priori knowledge of the topics present in the articles\n",
    "        self.models = [] #list of models\n",
    "    \n",
    "    # returns True is unread article contains new content, otherwise returns false\n",
    "    # \"unread\" article must be the last row of the dataframe (so if ordered by time, this would be the most recent article)\n",
    "    def predict(self,df,include_stop_ngrams=True):\n",
    "        current_topics = self.init_topics\n",
    "        level = 0 #current level of the model\n",
    "        \n",
    "        if current_topics == -1:\n",
    "            current_topics = min(20,min(math.ceil(df.shape[0]/3) + 1, df.shape[0]))\n",
    "            \n",
    "        for i in range(self.depth):\n",
    "            #perform fitting and transforming\n",
    "            vect = TfidfVectorizer(sublinear_tf=True,ngram_range=(1,3),max_df=0.5).fit(df['body_processed'])\n",
    "            X_train = None\n",
    "            \n",
    "            #remove/not remove top stop-ngrams\n",
    "            if not include_stop_ngrams:\n",
    "                X_train = self.remove_stop_ngrams(current_topics,df,vect)\n",
    "                include_stop_tokens = True\n",
    "            else:\n",
    "                X_train = vect.transform(df['body_processed'])\n",
    "            \n",
    "            lda = LatentDirichletAllocation(n_components=current_topics,random_state=0,max_iter=400,learning_method='batch')\n",
    "            lda.fit(X_train)\n",
    "            transformed = lda.transform(X_train)\n",
    "            \n",
    "            #save model\n",
    "            model = {}\n",
    "            model['lda'] = lda\n",
    "            model['transformed'] = transformed\n",
    "            model['topics'] = current_topics\n",
    "            model['vect'] = vect\n",
    "            model['X_train'] = X_train\n",
    "            model['df'] = df\n",
    "            self.models.append(model)\n",
    "            \n",
    "            #assign the highest-weighted topic as the main topic\n",
    "            topic_result = {}\n",
    "            for i in range(len(transformed)):\n",
    "                max_num = -99\n",
    "                max_index = -99\n",
    "                for j in range(len(transformed[i])):\n",
    "                    if transformed[i][j] >= max_num:\n",
    "                        max_num = transformed[i][j]\n",
    "                        max_index = j\n",
    "                if max_index not in topic_result:\n",
    "                    topic_result[max_index] = []\n",
    "                topic_result[max_index].append(i)\n",
    "                \n",
    "            index = df.shape[0]-1 #index of the \"unread\" article\n",
    "            subtopic = None #main topic assigned to \"unread\" article\n",
    "            for t in topic_result:\n",
    "                for a in topic_result[t]:\n",
    "                    if a == index:\n",
    "                        subtopic=t\n",
    "                        break\n",
    "            \n",
    "            #return true if the topic assigned to the \"unread\" article is unique across all \"read\" articles\n",
    "            if len(topic_result[subtopic]) == 1:\n",
    "                self.depth = level+1\n",
    "                return True                                \n",
    "                            \n",
    "            if i != self.depth-1:\n",
    "                df = df.iloc[topic_result[subtopic]] #reduce dataset to only articles assigned as the same topic as the \"unread\" article\n",
    "                current_topics = min(20,min(math.ceil(df.shape[0]/3) + 1, df.shape[0]))\n",
    "                level += 1\n",
    "\n",
    "        return False\n",
    "     \n",
    "    #removes the stop ngrams (i.e. most frequent ngrams), function not used in our final model\n",
    "    def remove_stop_ngrams(self,current_topics,df,vect):\n",
    "        X_train = vect.transform(df['body_processed'])\n",
    "        lda = LatentDirichletAllocation(n_components=current_topics,random_state=0,max_iter=400,learning_method='batch')\n",
    "        lda.fit(X_train)\n",
    "        transformed = lda.transform(X_train)\n",
    "        \n",
    "        topic_val = [0] * current_topics\n",
    "        for t in transformed:\n",
    "            for i in range(len(t)):\n",
    "                topic_val[i] += t[i]\n",
    "                \n",
    "        outlier_indexes = self.outlier_detection(topic_val)\n",
    "        \n",
    "        #remove stop tokens from X_train\n",
    "        df_vec = pd.DataFrame(X_train.toarray())\n",
    "        df_vec.columns = vect.get_feature_names()\n",
    "        for topic_index in outlier_indexes:\n",
    "            top_tokens = self.get_top_ngrams(lda,vect.get_feature_names(),topic_index)\n",
    "            df_vec.drop(top_tokens, axis=1, inplace=True)\n",
    "            \n",
    "        X_train_processed = scipy.sparse.csr_matrix(df_vec.values)\n",
    "        return X_train_processed\n",
    "    \n",
    "    #get top ngrams in topic\n",
    "    def get_top_ngrams(self,lda_model, feature_names, topic,n_top_tokens=20):\n",
    "        word_distrib = lda_model.components_[topic]\n",
    "        sorted_index = word_distrib.argsort()[:-n_top_tokens - 1:-1]\n",
    "        top_words = []\n",
    "        \n",
    "        for i in sorted_index:\n",
    "            top_words.append(feature_names[i])\n",
    "    \n",
    "        return top_words\n",
    "    \n",
    "    #detect outlier based on IQR\n",
    "    def outlier_detection(self,arr):\n",
    "        q1 = np.percentile(arr, 25)\n",
    "        q3 = np.percentile(arr, 75)\n",
    "        iqr = q3 - q1\n",
    "        ceiling = q3 + 1.5*iqr\n",
    "        outlier_indexes = []\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] > ceiling:\n",
    "                outlier_indexes.append(i)\n",
    "        return outlier_indexes\n",
    "    \n",
    "    # plot bar chart of topic distrubution\n",
    "    def stacked_barplot(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "            \n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]\n",
    "            transformed = current_model['transformed']\n",
    "            transformed_copy = copy.deepcopy(transformed)\n",
    "            topics = current_model['topics']\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.xlabel('Topics')\n",
    "            plt.ylabel('Topic Weight')\n",
    "            plt.xticks(np.arange(0,topics+1,1))\n",
    "            plt.title('Topic Distribution by Article at Depth ' + str(i+1))\n",
    "            \n",
    "            x=range(0, topics)\n",
    "            prev = None\n",
    "            first = True\n",
    "            width = 0.5\n",
    "            bars = []\n",
    "            for i in range(len(transformed_copy)): \n",
    "                p = plt.bar(x, transformed_copy[i], width, bottom=prev)\n",
    "                if first:\n",
    "                    prev = transformed_copy[i]\n",
    "                    first = False\n",
    "                else:\n",
    "                    prev += transformed_copy[i]\n",
    "                bars.append(p)\n",
    "            \n",
    "            #this is to avoid having a big legend that would cover the graph \n",
    "            if len(transformed_copy) <= 15:\n",
    "                articles_num = range(1,len(transformed_copy)+1)\n",
    "                plt.legend([p[0] for p in bars],[a for a in articles_num])\n",
    "    \n",
    "    # plot pyldavis\n",
    "    def pyldavis(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "            \n",
    "        pyLDAvis.enable_notebook()\n",
    "        for i in range(depth):\n",
    "            print(\"PyLDAvis at depth: \" + str(i+1))\n",
    "            current_model = self.models[i]\n",
    "            panel = pyLDAvis.sklearn.prepare(current_model['lda'], current_model['X_train'], current_model['vect'], mds='tsne')\n",
    "            display(panel)\n",
    "    \n",
    "    #get the topic assignment for each article\n",
    "    def topic_distribution(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        print(\"************************************************************\")\n",
    "        \n",
    "        for z in range(depth):\n",
    "            current_model = self.models[z]\n",
    "            transformed = current_model['transformed'] \n",
    "            print(\"Topic distribution at depth: \" + str(z+1))\n",
    "            print()\n",
    "\n",
    "            for i in range(len(transformed)):\n",
    "                max_num = -99\n",
    "                max_index = -99\n",
    "                \n",
    "                for j in range(len(transformed[i])):\n",
    "                    if transformed[i][j] >= max_num:\n",
    "                        max_num = transformed[i][j]\n",
    "                        max_index = j\n",
    "                print('max topic of doc ' + str(i) + ' is: ' + str(max_index))\n",
    "                \n",
    "            print(\"*********************************************************\")\n",
    "    \n",
    "    #helper function from Prof. Vosoughi to print the word distribution\n",
    "    def print_LDA_results(self,lda_model, feature_names, n_top_words=50):\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            message = \"Topic %d: \" % topic_idx\n",
    "            message += \" \".join([(\"*\" + feature_names[i] + \"*\")\n",
    "                                 for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "            print(message)\n",
    "            print()\n",
    "    \n",
    "    #print the word distribution (in descending weight) for each topic\n",
    "    def print_topics(self,depth=-1):\n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        print(\"************************************************************\")\n",
    "        for i in range(depth):\n",
    "            print(\"Topics at depth: \" + str(i+1))\n",
    "            print()\n",
    "            \n",
    "            current_model = self.models[i]\n",
    "            lda = current_model['lda']\n",
    "            vect = current_model['vect']\n",
    "            self.print_LDA_results(lda,vect.get_feature_names())\n",
    "            print(\"************************************************************\")\n",
    "            \n",
    "    def word_cloud(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        \n",
    "        for i in range(depth):\n",
    "            print(\"Word clouds at depth: \" + str(i+1))\n",
    "            current_model = self.models[i]\n",
    "            df = current_model['df']\n",
    "            df_old = df.iloc[:df.shape[0]-1]\n",
    "            words = \"\"\n",
    "            for text in df_old['body_processed']:\n",
    "                words+= \" \" + text\n",
    "                \n",
    "            #word cloud of all \"read\" articles\n",
    "            wordcloud = WordCloud().generate(words)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            \n",
    "            #word cloud of the \"unread\" article\n",
    "            words = df.iloc[df.shape[0]-1]['body_processed']\n",
    "            wordcloud = WordCloud().generate(words)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.figure() \n",
    "            print(\"************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca = NCA(depth=2)\n",
    "if nca.predict(df):\n",
    "    print('The unread article contains new content')\n",
    "else:\n",
    "    print('The unread article does not contain new content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.stacked_barplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.topic_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.word_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.pyldavis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
