{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read articles from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = None\n",
    "filename = 'YOUR_FILE_NAME.txt'\n",
    "folder_name = 'YOUR_FOLDER_NAME'\n",
    "with open(folder_name + '/' + filename) as json_file:  \n",
    "    articles = json.load(json_file)\n",
    "df = pd.DataFrame(articles)\n",
    "df['date'] =pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "pattern = re.compile('^[$€]?\\d+(,\\d+)*.?\\d*[,.]?$')\n",
    "\n",
    "def matchesNum(term):\n",
    "    if re.match(pattern, term):\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def removePunctuation(term):\n",
    "    return term.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def updateNumbers(text):\n",
    "    split_text = text.split(' ')\n",
    "    updated_split_text=[]\n",
    "    term_num=len(split_text)\n",
    "    i = 0\n",
    "    while i < term_num:\n",
    "        if matchesNum(split_text[i]):\n",
    "        \n",
    "            if (i+1 < term_num) and (removePunctuation(split_text[i+1])=='million' or removePunctuation(split_text[i+1])=='billion'):\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))+removePunctuation(split_text[i+1])\n",
    "                i+=1\n",
    "            else:\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))\n",
    "            new_word = new_word.rstrip('.')\n",
    "            new_word = new_word.replace('.','a')\n",
    "            new_word = new_word.replace('$','d')\n",
    "            new_word = new_word.replace('€','eur')\n",
    "            updated_split_text.append(new_word)\n",
    "        else:\n",
    "            updated_split_text.append(split_text[i])\n",
    "        i+=1\n",
    "    return ' '.join(updated_split_text)\n",
    "\n",
    "def process_text(text):\n",
    "    updated_text = updateNumbers(text)\n",
    "    words = nltk.word_tokenize(updated_text)\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w.translate(str.maketrans('', '', string.punctuation)) for w in words]\n",
    "    words = [w for w in words if w not in nltk.corpus.stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if w.isalnum()]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Process \"body\" of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_processed'] = [process_text(b) for b in df['body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Split articles (i.e. old articles vs new article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new =df.iloc[:1,:]\n",
    "# df_old = df.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sort articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.index = df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.a Ascending (oldest to newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.b Descending (newest to oldest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sort_index(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
