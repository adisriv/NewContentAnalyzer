{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from eventregistry import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pull from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_API_KEY = '4a2c424f-0a05-4691-b59b-7b7338b3ba9e'\n",
    "er = EventRegistry(apiKey = MY_API_KEY)\n",
    "# q = QueryArticlesIter(conceptUri = er.getConceptUri(\"Xi Jinping\"),lang=\"eng\",dataType=\"news\",sourceUri=QueryItems.OR([\n",
    "#     er.getNewsSourceUri(\"nytimes.com\"),\n",
    "#     er.getNewsSourceUri(\"wsj.com\")\n",
    "# ]))\n",
    "q = QueryArticlesIter(conceptUri = er.getConceptUri(\"trade war\"),lang=\"eng\",dataType=\"news\",sourceUri=\"nytimes.com\")\n",
    "\n",
    "articles = []\n",
    "first = True\n",
    "bad_prefix = set()\n",
    "bad_prefix.add(\"On Politics: The Biggest Stories of the Week\")\n",
    "bad_prefix.add(\"DealBook Briefing:\")\n",
    "bad_suffix = set()\n",
    "bad_suffix.add(\"Briefing\")\n",
    "\n",
    "size = 35\n",
    "counter = 0\n",
    "for art in q.execQuery(er, sortBy = \"date\", maxItems=60):\n",
    "    if size == counter:\n",
    "        break\n",
    "    if len(art['body']) > 3000 and not (art['title'].startswith(tuple(bad_prefix)) or art['title'].endswith(tuple(bad_suffix))):\n",
    "        counter += 1\n",
    "        articles.append(art)\n",
    "\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. Pull article manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = {}\n",
    "# article['body'] = \"\"\n",
    "# article['date'] = \"YYYY-MM-DD\"\n",
    "# article['url'] = \"\"\n",
    "# article['title'] = \"\"\n",
    "\n",
    "# #Deault values\n",
    "# article['authors'] = []\n",
    "# article['dataType'] = 'news'\n",
    "# article['dateTime'] = None\n",
    "# article['eventUri'] = None\n",
    "# article['image'] = None\n",
    "# article['idDuplicate'] = False\n",
    "# article['lang'] = \"eng\"\n",
    "# article['sentiment'] = None\n",
    "# article['sim'] = None\n",
    "# article['source'] = None\n",
    "# article['time'] = None\n",
    "# article['uri'] = None\n",
    "# article['wgt'] = None\n",
    "# new_articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([pd.DataFrame(new_articles), df], ignore_index=True) #places the new articles on top of the articles in the existing dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def process_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w.translate(str.maketrans('', '', string.punctuation)) for w in words]\n",
    "    words = [w for w in words if w not in nltk.corpus.stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if w.isalnum()] \n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Process \"body\" of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = [process_text(b) for b in df['body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Split articles (i.e. old articles vs new article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new =df.iloc[:1,:]\n",
    "# df_old = df.iloc[1:,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
