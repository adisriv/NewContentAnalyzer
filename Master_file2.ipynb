{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read articles from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = None\n",
    "filename = '52_articles(working).txt'\n",
    "folder_name = 'Local_dataset'\n",
    "with open(folder_name + '/' + filename) as json_file:  \n",
    "    articles = json.load(json_file)\n",
    "df = pd.DataFrame(articles)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "pattern = re.compile('^[$€]?[0-9]+(,[0-9]+)*.?[0-9]*[,.]?$')\n",
    "\n",
    "def matchesNum(term):\n",
    "    if re.match(pattern, term):\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def removePunctuation(term):\n",
    "    return term.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def updateNumbers(text):\n",
    "    split_text = text.split(' ')\n",
    "    updated_split_text=[]\n",
    "    term_num=len(split_text)\n",
    "    i = 0\n",
    "    while i < term_num:\n",
    "        if matchesNum(split_text[i]):\n",
    "        \n",
    "            if (i+1 < term_num) and (removePunctuation(split_text[i+1])=='million' or removePunctuation(split_text[i+1])=='billion'):\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))+removePunctuation(split_text[i+1])\n",
    "                i+=1\n",
    "            else:\n",
    "                new_word = split_text[i].translate(str.maketrans('', '', ','))\n",
    "            new_word = new_word.rstrip('.')\n",
    "            new_word = new_word.replace('.','dot')\n",
    "            new_word = new_word.replace('$','dol')\n",
    "            new_word = new_word.replace('€','eur')\n",
    "            updated_split_text.append(new_word)\n",
    "        else:\n",
    "            updated_split_text.append(split_text[i])\n",
    "        i+=1\n",
    "    return ' '.join(updated_split_text)\n",
    "\n",
    "def process_text(text):\n",
    "    updated_text = updateNumbers(text)\n",
    "    words = nltk.word_tokenize(updated_text)\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w.translate(str.maketrans('', '', string.punctuation)) for w in words]\n",
    "    words = [w for w in words if w not in nltk.corpus.stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if w.isalnum()]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Process \"body\" of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_processed'] = [process_text(b) for b in df['body']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Split articles (i.e. old articles vs new article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new =df.iloc[:1,:]\n",
    "# df_old = df.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] =pd.to_datetime(df['date'])\n",
    "df.index = df['date']\n",
    "df = df.sort_index(ascending=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCA:\n",
    "    \n",
    "    def __init__(self,depth=1,init_topics=-1):\n",
    "        self.depth = depth\n",
    "        self.init_topics = init_topics\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self,df):\n",
    "        current_topics = self.init_topics\n",
    "        if current_topics == -1:\n",
    "            current_topics = int(df.shape[0]/3) + 1\n",
    "    \n",
    "        for i in range(self.depth):\n",
    "            \n",
    "            #perfom fitting and transforming\n",
    "            vect = TfidfVectorizer(sublinear_tf=True,ngram_range=(1,3),max_df=0.7).fit(df['body_processed'])\n",
    "            X_train = vect.transform(df['body_processed'])\n",
    "            lda = LatentDirichletAllocation(n_components=current_topics,random_state=0,max_iter=400,learning_method='batch')\n",
    "            lda.fit(X_train)\n",
    "            transformed = lda.transform(X_train)\n",
    "            \n",
    "            #save model\n",
    "            model = {}\n",
    "            model['lda'] = lda\n",
    "            model['transformed'] = transformed\n",
    "            model['topics'] = current_topics\n",
    "            model['vect'] = vect\n",
    "            model['X_train'] = X_train\n",
    "            self.models.append(model)\n",
    "            \n",
    "            topic_result = {}\n",
    "            for i in range(len(transformed)):\n",
    "                max_num = -99\n",
    "                max_index = -99\n",
    "                for j in range(len(transformed[i])):\n",
    "                    if transformed[i][j] >= max_num:\n",
    "                        max_num = transformed[i][j]\n",
    "                        max_index = j\n",
    "                if max_index not in topic_result:\n",
    "                    topic_result[max_index] = []\n",
    "                topic_result[max_index].append(i)\n",
    "                \n",
    "            if i != self.depth-1:\n",
    "                index = df.shape[0]-1\n",
    "                subtopic = None\n",
    "                for t in topic_result:\n",
    "                    for a in topic_result[t]:\n",
    "                        if a == index:\n",
    "                            subtopic=t\n",
    "                            break\n",
    "\n",
    "                df = df.iloc[topic_result[subtopic]]\n",
    "                current_topics = int(df.shape[0]/3) + 1\n",
    "            \n",
    "    def stacked_barplot(self,depth=-1):\n",
    "        \n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        \n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]\n",
    "            transformed = current_model['transformed']\n",
    "            transformed_copy = list(transformed)\n",
    "            topics = current_model['topics']\n",
    "            \n",
    "            plt.figure()\n",
    "            x=range(0, topics)\n",
    "            prev = None\n",
    "            first = True\n",
    "            width = 0.5\n",
    "            bars = []\n",
    "            for i in range(len(transformed_copy)): \n",
    "                p = plt.bar(x, transformed_copy[i], width, bottom=prev)\n",
    "                if first:\n",
    "                    prev = transformed_copy[i]\n",
    "                    first = False\n",
    "                else:\n",
    "                    prev += transformed_copy[i]\n",
    "            bars.append(p)\n",
    "            if len(transformed_copy) <= 15:\n",
    "                articles_num = range(1,len(transformed_copy)+1)\n",
    "                plt.legend([p[0] for p in bars],[a for a in articles_num])\n",
    "                \n",
    "    def pyldavis(self,depth=-1):\n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        \n",
    "        pyLDAvis.enable_notebook()\n",
    "        for i in range(depth):\n",
    "            current_model = self.models[i]   \n",
    "            panel = pyLDAvis.sklearn.prepare(current_model['lda'], current_model['X_train'], current_model['vect'], mds='tsne')\n",
    "            panel\n",
    "            \n",
    "    def print_maxtopic(self,depth=-1):\n",
    "        if depth == -1:\n",
    "            depth = self.depth\n",
    "        print(\"************************************************************\")\n",
    "        for z in range(depth):\n",
    "            current_model = self.models[z]\n",
    "            transformed = current_model['transformed']            \n",
    "            for i in range(len(transformed)):\n",
    "                max_num = -99\n",
    "                max_index = -99\n",
    "                for j in range(len(transformed[i])):\n",
    "                    if transformed[i][j] >= max_num:\n",
    "                        max_num = transformed[i][j]\n",
    "                        max_index = j\n",
    "                print('max topic of doc ' + str(i) + ' is: ' + str(max_index))\n",
    "                \n",
    "            print(\"*********************************************************\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca = NCA(depth=2)\n",
    "nca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.print_maxtopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.stacked_barplot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
